How well did it do? What, if anything, can you learn from this?

    I think there are some problems in the algorithm. Because when I vote for the final test data, there are only two circumstances. One is the instance in test data is the same as the training data, then the result of vote is all for positive/negative and null for the other. The second circumstances is that the instance in test data is a new input, and the vote of classification for positive and negative is precisely half and half. So, this algorithm actually cannot tell the final result of classification.

    In my opinion, since the training data is considered to be correct, it has obviously no relationship with training data. Then, if I use the hypothesis space in partA (FindS algorithm), I find the version space shrinks to zero soon after a few training data. This might because we use strong assumptions before this algorithm, to reduce the number of hypothesis from 65536 to 82. However, if I let the size of hypothesis space be equal to the size of concept space, then I get the problem stated in the first paragraph, with equally voting. Actually, whatever the values of each feature(2 or 3 or 4), the size of remaining version space must be even and the voting must be equally. I think the reason is we use no prior assumptions, or inductive bias before the algorithm. Then for any hypothesis h in H that has one prediction on one input x, there must be another h’in H that has an opposite classification result on x with all the other inputs the same result. Thus, the voting result must be equally. In conclusion, when making no assumption before classification, there surely covers the whole concept space, but actually has no meaning in prediction. However, if the assumption is too strong to making the hypothesis space quite small, the algorithm is also not good for the reason of leaving some “correct”answers. So before thinking of an algorithm, it is quite important to make reasonable assumptions.
